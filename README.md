# Transformer Machine Translation (English-Chinese)

è¿™æ˜¯ä¸€ä¸ªåŸºäº PyTorch å®ç°çš„ Transformer æœºå™¨ç¿»è¯‘é¡¹ç›®ï¼Œæ—¨åœ¨ä»é›¶å¼€å§‹æ„å»º Transformer æ¶æ„ï¼Œå®ç°è‹±è¯­åˆ°ä¸­æ–‡çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚æœ¬é¡¹ç›®é€‚åˆç”¨äºå­¦ä¹  Transformer çš„å†…éƒ¨æœºåˆ¶ã€æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠåºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰æ¨¡å‹çš„è®­ç»ƒæµç¨‹ã€‚

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®Œæ•´å¤ç°äº† "Attention Is All You Need" è®ºæ–‡ä¸­çš„ Transformer æ¨¡å‹ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹ç‰¹æ€§ï¼š

*   **æ ¸å¿ƒæ¶æ„**ï¼šå®ç°äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰ã€ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ã€å‰é¦ˆç¥ç»ç½‘ç»œç­‰æ ¸å¿ƒç»„ä»¶ã€‚
*   **æ•°æ®å¤„ç†**ï¼šä½¿ç”¨ SentencePiece è¿›è¡Œ BPE åˆ†è¯ï¼Œæ”¯æŒåŠ¨æ€æ•°æ®åŠ è½½ä¸ Padding Mask / Sequence Mask å¤„ç†ã€‚
*   **è®­ç»ƒæµç¨‹**ï¼šåŒ…å«å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€éªŒè¯é›†è¯„ä¼°ã€æŸå¤±è®¡ç®—ï¼ˆLabel Smoothingï¼‰ä»¥åŠåŸºäº BLEU åˆ†æ•°çš„æ¨¡å‹ä¿å­˜æœºåˆ¶ã€‚
*   **è§£ç ç­–ç•¥**ï¼šå®ç°äº† Beam Searchï¼ˆæŸæœç´¢ï¼‰ç®—æ³•ï¼Œæå‡ç¿»è¯‘è´¨é‡ã€‚
*   **å·¥ç¨‹å®è·µ**ï¼šæ”¯æŒå¤š GPU å¹¶è¡Œè®­ç»ƒï¼ˆDataParallelï¼‰ï¼Œé›†æˆ TensorBoardï¼ˆå¯é€‰ï¼‰æˆ–æ—¥å¿—è®°å½•ï¼Œä»£ç ç»“æ„æ¸…æ™°æ¨¡å—åŒ–ã€‚

## ğŸ› ï¸ ç¯å¢ƒä¾èµ–

è¯·ç¡®ä¿å®‰è£…äº† Python 3.8+ï¼Œå¹¶å®‰è£…ä»¥ä¸‹ä¾èµ–åº“ï¼š

```bash
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–åŒ…æ‹¬ï¼š
*   torch >= 1.12.1
*   numpy
*   sentencepiece
*   sacrebleu (ç”¨äº BLEU è¯„æµ‹)
*   tqdm

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
Transformer01/
â”œâ”€â”€ main.py                 # è®­ç»ƒä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ translate.py            # ç¿»è¯‘/æ¨ç†è„šæœ¬
â”œâ”€â”€ config.py               # å…¨å±€é…ç½®æ–‡ä»¶ï¼ˆè¶…å‚æ•°ã€è·¯å¾„ç­‰ï¼‰
â”œâ”€â”€ beam_decoder.py         # Beam Search è§£ç å®ç°
â”œâ”€â”€ requirements.txt        # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ model/                  # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ tf_model.py         # Transformer æ¨¡å‹æ ¸å¿ƒä»£ç 
â”‚   â””â”€â”€ train_utils.py      # è®­ç»ƒè¾…åŠ©å·¥å…·ï¼ˆLossè®¡ç®—ç­‰ï¼‰
â”œâ”€â”€ data/                   # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ json/               # å­˜æ”¾è®­ç»ƒæ•°æ® (train.json, dev.json, test.json)
â”‚   â”œâ”€â”€ get_corpus.py       # è¯­æ–™é¢„å¤„ç†è„šæœ¬
â”‚   â””â”€â”€ analyze_corpus.py   # è¯­æ–™åˆ†æè„šæœ¬
â”œâ”€â”€ tokenizer/              # åˆ†è¯å™¨æ¨¡å‹
â”‚   â”œâ”€â”€ chn.model / .vocab  # ä¸­æ–‡ SentencePiece æ¨¡å‹
â”‚   â””â”€â”€ eng.model / .vocab  # è‹±æ–‡ SentencePiece æ¨¡å‹
â””â”€â”€ tools/                  # å·¥å…·å‡½æ•°
    â”œâ”€â”€ data_loader.py      # Dataset å’Œ DataLoader å®ç°
    â””â”€â”€ ...
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ•°æ®å‡†å¤‡
ç¡®ä¿ `data/json/` ç›®å½•ä¸‹åŒ…å« `train.json`, `dev.json`, `test.json` æ•°æ®é›†æ–‡ä»¶ã€‚æ•°æ®æ ¼å¼ä¸º JSON åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« `[è‹±æ–‡å¥å­, ä¸­æ–‡å¥å­]`ã€‚

### 2. è®­ç»ƒæ¨¡å‹
è¿è¡Œ `main.py` å¼€å§‹è®­ç»ƒï¼š

```bash
python main.py
```

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼š
*   æ¨¡å‹ä¼šè‡ªåŠ¨åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚
*   æ ¹æ® BLEU åˆ†æ•°è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° `experiments/` ç›®å½•ã€‚
*   æ—¥å¿—ä¼šè¾“å‡ºå½“å‰çš„ Loss å’Œ BLEU å¾—åˆ†ã€‚

å¯ä»¥åœ¨ `config.py` ä¸­ä¿®æ”¹è¶…å‚æ•°ï¼Œä¾‹å¦‚ï¼š
*   `batch_size`: æ‰¹æ¬¡å¤§å°
*   `epoch_num`: è®­ç»ƒè½®æ•°
*   `d_model`, `n_layers`, `n_heads`: æ¨¡å‹æ¶æ„å‚æ•°

### 3. æ¨¡å‹æ¨ç†ï¼ˆç¿»è¯‘ï¼‰
ä½¿ç”¨ `translate.py` è¿›è¡Œå•å¥ç¿»è¯‘æµ‹è¯•ã€‚è¯¥è„šæœ¬ä¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶æä¾›äº¤äº’å¼è¾“å…¥ï¼š

```bash
python translate.py
```

è¾“å…¥è‹±æ–‡å¥å­å³å¯è·å¾—ä¸­æ–‡ç¿»è¯‘ç»“æœã€‚

## âš™ï¸ é…ç½®è¯´æ˜

æ‰€æœ‰ä¸»è¦é…ç½®å‡åœ¨ `config.py` ä¸­å®šä¹‰ï¼ŒåŒ…æ‹¬ï¼š

*   **æ¨¡å‹å‚æ•°**ï¼š`d_model` (512), `n_heads` (8), `n_layers` (6) ç­‰ã€‚
*   **è®­ç»ƒå‚æ•°**ï¼š`batch_size` (32), `lr` (3e-4), `dropout` (0.1)ã€‚
*   **ç”Ÿæˆå‚æ•°**ï¼š`beam_size` (3), `max_len` (60)ã€‚
*   **è·¯å¾„é…ç½®**ï¼šæ•°æ®è·¯å¾„ã€æ¨¡å‹ä¿å­˜è·¯å¾„ç­‰ã€‚

## ğŸ“ å‚è€ƒèµ„æ–™

*   [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸè®ºæ–‡
*   [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - å“ˆä½› NLP å›¢é˜Ÿçš„ä»£ç è§£è¯»

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue æˆ– Pull Request æ¥æ”¹è¿›æœ¬é¡¹ç›®ï¼
